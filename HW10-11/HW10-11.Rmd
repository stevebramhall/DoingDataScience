---
title: "DDS HW10-11"
author: "Steve Bramhall"
date: "November 10, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE}
library(dplyr)      # for string functions
library(stringr)    # for str_trim function
library(caret)      # for classification & regression training
library(mlr)        # for machine learing
library(class)      # for classification funtions
#library(FNN)        # for knn.reg function
```


###A. Clean and prepare the 

####1. First read the two data sets into data frames beerDF and breweryDF from csv files. The columns (variables) of each data frame were custom named to be more readable and to provide a handle to merge the data.Create column for Brewery ID that is common to both datasets. Create column for brewery ID that is common to both datasets similar to what you did in the project. So we can merge!
```{r echo = TRUE}
# Read beer data from csv file and put into breweryDF data frame.Then rename columns to be more readable.
beerDF <- read.csv(".\\InputFiles\\Beers.csv",header=TRUE,sep=",",stringsAsFactors = FALSE, encoding = "UTF-8")
names(beerDF) <- c("BeerName","BeerID","ABV","IBU","BreweryID","BeerStyle","Ounces") # rename table columns

# Read brewery data from csv file and put into breweryDF data frame.Then rename columns to be more readable.
breweryDF <- read.csv(".\\InputFiles\\Breweries.csv",header=TRUE,sep=",",stringsAsFactors = FALSE)
names(breweryDF) <- c("BreweryID","BreweryName","City","State") # rename table columns
```

####2. Merge the beer and brewery data sets into a sigle dataframe.
```{r echo = TRUE}
#Merge brewery and beer data into the brewDataDF data frame
allBeerDataDF <- merge.data.frame(breweryDF,beerDF,by="BreweryID")
```

####3. Remove surrounding whitespace for State info
```{r echo = TRUE}
allBeerDataDF$State <- sapply(allBeerDataDF$State, str_trim)    # remove surrounding whitespace to state col
```

####4. Create a separate data set with only CO & TX beer and remove IBU's with NA's.
```{r echo = TRUE}
beerCOTX=subset(allBeerDataDF,(allBeerDataDF$State=="CO" | allBeerDataDF$State=="TX") & !is.na(IBU))
```

####5. Order beerCOTX by IBU (ascending)
```{r echo = TRUE}
beerCOTX <- beerCOTX[with(beerCOTX,order(IBU)),] # sort by IBU, ascending
beerCOTX$IBU <- sapply(beerCOTX$IBU,as.numeric)  # make IBU numeric
```

####6. For this assignment we will concentrate only on the Texas data!  Create a training and test set from the data (60%/40% split respectively).  Print a summary of each new data frame… there should be two: TrainingTX, TestTX.  
```{r}
# creating subset for TX, easier to work with
beerTX=subset(beerCOTX,beerCOTX$State=="TX") 
beerTX=data.frame(IBU=beerTX$IBU,ABV=beerTX$ABV,Style=beerTX$BeerStyle)
beerTX$Style = as.character(beerTX$Style)

#Divide TX into training and test set into 60% training & 40% test
samplesizeTX=nrow(beerTX)
train_percent = .6
train_indicesTX = sample(seq(1,samplesizeTX,length = samplesizeTX),train_percent*samplesizeTX) # get random indices
trainingTX = beerTX[train_indicesTX,] # TX random training data from original data
testTX = beerTX[-train_indicesTX,] # TX random test data from original data

#This code code is used to check the training and test data sets
length.train = length(trainingTX$IBU)
length.test = length(testTX$IBU)

percent.test = length.test/(length.test+length.train)
percent.train = length.train/(length.test+length.train)
percent.test
percent.train

summarizeColumns(trainingTX)
summarizeColumns(testTX)
```

####7. Using the training data, fit a KNN regression model to predict ABV from IBU.  You should use the knnreg function in the caret package.  Fit two separate models: one with k = 3 and one with k = 5.  (This is 2 models total.)
```{r}

plot(trainingTX$IBU,trainingTX$ABV,main = "KNN")
text(trainingTX$IBU,trainingTX$ABV,labels = trainingTX$IBU,pos=4,col="blue")
points(testTX$IBU,testTX$ABV,col="red",pch=15)

trainingTX.x=data.frame(IBU=trainingTX$IBU)      # Must be DF since there could be multiple explanatory variables
trainingTX.y=trainingTX$ABV                      # Must be a vector 
testTX.x=data.frame(IBU=testTX$IBU)              # Should be DF in case more explanatory variables are added

# knnreg(explanatoryTrainingDF,responseTraining,k)
fit.k3=knnreg(x=trainingTX.x,y=trainingTX.y,k=3) 
fit.k5=knnreg(x=trainingTX.x,y=trainingTX.y,k=5)

# predict(model,explanatoryTestDF)
knn.k3=predict(fit.k3,testTX.x)                  
knn.k5=predict(fit.k5,testTX.x)


par(mfrow=c(1,2))
plot(knn.k3,testTX$ABV,main = "KNN-k3")
plot(knn.k5,testTX$ABV,main = "KNN-k5")
```

####8. Use the ASE loss function and external cross validation to provide evidence as to which model (k = 3 or k = 5) is more appropriate. Remember your answer should be supported with why you feel a certain model is appropriate. Your analysis should include the average squared error (ASE) for each model from the test set. Your analysis should also include a clear discussion, using the ASEs, as to which model you feel is more appropriate.

```{r}
ASE.k3=((sum(knn.k3-testTX$ABV))^2)/length(testTX$ABV)
ASE.k5=((sum(knn.k5-testTX$ABV))^2)/length(testTX$ABV)
```

##### Both models produced great results but the k=3 KNN model produced better results than the k=5.

####9. Now use the ASE loss function and external cross validation to provide evidence as to which model (the linear regression model from last week or the “best” KNN regression model from this week (from question 10)) is more appropriate. 

```{r}
#fit the first model to the training set
linear.fit.TX = lm(ABV~IBU, data = trainingTX)

#get predictions for the test data from the model fit on the training data
linear.predict.TX = predict(linear.fit.TX,newdata = testTX.x)

#Calculate the RMSE loss function
ASE.linear=((sum((testTX$ABV-linear.predict.TX)^2)/length(testTX$ABV))) 

ASE.k3
ASE.k5
ASE.linear

```

#### KNN, k=3 model works best.

####10. Use your “best” KNN regression model to predict the ABV for an IBU of 150, 170 and 190.  What issue do you see with using KNN to extrapolate?    

```{r}
knn.k3=predict(fit.k3,c(150,170,190))
knn.k3
```

####11. Filter the beerCOTX dataframe for only beers that are from Texas and are American IPA and American Pale Ale.
```{r}
# Get Beer Style Indices
IPA.indices=grep("American IPA", beerTX$Style)
Ale.indices=grep("American Pale Ale", beerTX$Style)
IPA.Ale.indices = append(IPA.indices,Ale.indices)

beerTX2 = beerTX[IPA.Ale.indices,]
factor(beerTX2)
```

####12. Divide this filtered data set into a training and test set (60/40, training / test split).  
```{r}
#Divide TX into training and test set into 60% training & 40% test
samplesizeTX2=nrow(beerTX2)
train_percent2 = .6
train_indicesTX2 = sample(seq(1,samplesizeTX2,length = samplesizeTX2),train_percent2*samplesizeTX2) # get random indices
trainingTX2 = beerTX2[train_indicesTX2,c(1,2)]   # TX random training data from original data
testTX2 = beerTX2[-train_indicesTX2,c(1,2)]      # TX random test data from original data
training.beerstyle = beerTX2[train_indicesTX2,3]
test.beerstyle = beerTX2[-train_indicesTX2,3]
```

####13. Use the class package’s knn function to build an KNN classifier with k = 3 that will use ABV and IBU as features (explanatory variables) to classify Texas beers as American IPA or American Pale Ale using the Training data.  Use your test set to create a confusion table to estimate the accuracy, sensitivity and specificity of the model.  
```{r}
#training.beerstyle =trainingTX2$Style
knn2.k3 = knn(trainingTX2,testTX2,cl=training.beerstyle,k=3)
```

####14. Using the same process as in the last question, find the accuracy, sensitivity and specificity of a KNN model with k = 5.  Which is “better”?  Why?
```{r}
knn2.k5 = knn(trainingTX2,testTX2,cl=training.beerstyle,k=5)
confusionMatrix(table(test.beerstyle,knn2.k5))

```

